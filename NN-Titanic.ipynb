{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "# Attempt to load TensorFlow\n",
    "  import tensorflow as tf\n",
    "  print(\"TensorFlow version:\", tf.__version__)\n",
    "  \n",
    "except Exception as e: \n",
    "# In case of an error, Tensorflow is installed in your environment\n",
    "  print(e)\n",
    "  !pip3 install tensorflow\n",
    "  print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-cc1b1dcfb301>:8: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  import pandas_profiling\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas_profiling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and get some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset (train and eval)\n",
    "dftrain = pd.read_csv(\n",
    "    'https://storage.googleapis.com/tf-datasets/titanic/train.csv')  # training data\n",
    "dfeval = pd.read_csv(\n",
    "    'https://storage.googleapis.com/tf-datasets/titanic/eval.csv')  # testing data\n",
    "\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "## Check if data are unbalanced or not\n",
    "print(np.size(np.where(y_train==0)))\n",
    "print(np.size(np.where(y_train==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We rebalance the classes\n",
    "\n",
    "# Instantiate the RandomOverSampler object\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = oversampler.fit_resample(dftrain, y_train)\n",
    "\n",
    "# Recreate the DataFrame with the rebalanced data\n",
    "dftrain = pd.DataFrame(X_resampled, columns=dftrain.columns)\n",
    "y_train = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "## Now the classes are balanced\n",
    "\n",
    "print(np.size(np.where(y_train==0)))\n",
    "print(np.size(np.where(y_train==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
      "0    male  22.0                   1      0   7.2500  Third  unknown   \n",
      "1  female  38.0                   1      0  71.2833  First        C   \n",
      "2  female  26.0                   0      0   7.9250  Third  unknown   \n",
      "3  female  35.0                   1      0  53.1000  First        C   \n",
      "4    male  28.0                   0      0   8.4583  Third  unknown   \n",
      "\n",
      "   embark_town alone  \n",
      "0  Southampton     n  \n",
      "1    Cherbourg     n  \n",
      "2  Southampton     y  \n",
      "3  Southampton     n  \n",
      "4   Queenstown     y  \n",
      "\n",
      "sex                    object\n",
      "age                   float64\n",
      "n_siblings_spouses      int64\n",
      "parch                   int64\n",
      "fare                  float64\n",
      "class                  object\n",
      "deck                   object\n",
      "embark_town            object\n",
      "alone                  object\n",
      "dtype: object\n",
      "\n",
      "              age  n_siblings_spouses       parch        fare\n",
      "count  768.000000          768.000000  768.000000  768.000000\n",
      "mean    29.509544            0.544271    0.391927   37.048458\n",
      "std     12.415038            1.079919    0.791933   56.851696\n",
      "min      0.750000            0.000000    0.000000    0.000000\n",
      "25%     23.000000            0.000000    0.000000    8.050000\n",
      "50%     28.000000            0.000000    0.000000   16.100000\n",
      "75%     35.000000            1.000000    0.000000   38.625000\n",
      "max     80.000000            8.000000    5.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "print(dftrain.head())\n",
    "print()\n",
    "print(dftrain.dtypes)\n",
    "print()\n",
    "print(dftrain.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset:  62%|██████▎   | 10/16 [00:01<00:01,  4.24it/s, Calculate auto correlation]         c:\\Users\\mfatt\\anaconda3\\lib\\site-packages\\multimethod\\__init__.py:315: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  return func(*args, **kwargs)\n",
      "Summarize dataset: 100%|██████████| 34/34 [00:09<00:00,  3.69it/s, Completed]                                     \n",
      "Generate report structure: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]\n",
      "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b415af941674c75bc95bf0b05574adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Render HTML: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 76.60it/s]\n"
     ]
    }
   ],
   "source": [
    "## We create an interactive dashboard to have some information on our data\n",
    "\n",
    "report = pandas_profiling.ProfileReport(dftrain, title='Exploratory Data Analysis Report',\n",
    "                                         minimal=False, html={'style':{'full_width':True}}\n",
    "                                        )\n",
    "\n",
    "report.to_widgets()\n",
    "\n",
    "report.to_file(\"report/report.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preprocessing to train a neural network with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_to_int_columns = ['n_siblings_spouses', 'parch'] \n",
    "\n",
    "## Convert columns of type object to int \n",
    "dftrain[object_to_int_columns] = dftrain[object_to_int_columns].astype(np.int64)\n",
    "dfeval[object_to_int_columns] = dfeval[object_to_int_columns].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int_columns = ['sex', 'class', 'deck', 'embark_town', 'alone'] \n",
    "\n",
    "## Convert columns of type object to string\n",
    "dftrain[string_to_int_columns] = dftrain[string_to_int_columns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize numeric features\n",
    "numeric_features = ['age', 'fare'] ## Numeric/continuous columns (float)\n",
    "\n",
    "### Preprocess numeric features\n",
    "numeric_features = dftrain[numeric_features].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The class tf.keras.Input is used to create \n",
    "## an input layer in a neural network model \n",
    "## built with the Keras library of TensorFlow.\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "for name, column in dftrain.items():\n",
    "\n",
    "    if type(column[0]) == str:\n",
    "        dtype = tf.string\n",
    "    elif type(column[0]) == np.int64:\n",
    "        dtype = tf.int64\n",
    "    else:\n",
    "        dtype = tf.float64\n",
    "\n",
    "    inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sex': <KerasTensor: shape=(None,) dtype=string (created by layer 'sex')>,\n",
       " 'age': <KerasTensor: shape=(None,) dtype=float64 (created by layer 'age')>,\n",
       " 'n_siblings_spouses': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'n_siblings_spouses')>,\n",
       " 'parch': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'parch')>,\n",
       " 'fare': <KerasTensor: shape=(None,) dtype=float64 (created by layer 'fare')>,\n",
       " 'class': <KerasTensor: shape=(None,) dtype=string (created by layer 'class')>,\n",
       " 'deck': <KerasTensor: shape=(None,) dtype=string (created by layer 'deck')>,\n",
       " 'embark_town': <KerasTensor: shape=(None,) dtype=string (created by layer 'embark_town')>,\n",
       " 'alone': <KerasTensor: shape=(None,) dtype=string (created by layer 'alone')>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_1')>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## preprocessed will be a list of tensors that will \n",
    "# contain information about our neural network input data\n",
    "preprocessed = []\n",
    "\n",
    "for name in numeric_features:\n",
    "    var = np.array(dftrain[name]).reshape(-1, 1)\n",
    "    # print(var)\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(var)\n",
    "    \n",
    "    x = inputs[name][:, tf.newaxis]\n",
    "    x = normalizer(x)\n",
    "    preprocessed.append(x)\n",
    "    \n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "\n",
    "for name in categorical_columns:\n",
    "\n",
    "    vocab = sorted(set(dftrain[name]))\n",
    "\n",
    "    if type(vocab[0]) is str:\n",
    "        lookup = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab, output_mode='one_hot')\n",
    "    else:\n",
    "        lookup = tf.keras.layers.IntegerLookup(\n",
    "            vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "    x = inputs[name][:, tf.newaxis]\n",
    "    x = lookup(x)\n",
    "    preprocessed.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_1')>,\n",
       " <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'string_lookup')>,\n",
       " <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'integer_lookup')>,\n",
       " <KerasTensor: shape=(None, 7) dtype=float32 (created by layer 'integer_lookup_1')>,\n",
       " <KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'string_lookup_1')>,\n",
       " <KerasTensor: shape=(None, 9) dtype=float32 (created by layer 'string_lookup_2')>,\n",
       " <KerasTensor: shape=(None, 5) dtype=float32 (created by layer 'string_lookup_3')>,\n",
       " <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'string_lookup_4')>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 41) dtype=float32 (created by layer 'model')>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
    "preprocessor = tf.keras.Model(inputs=inputs, outputs=preprocesssed_result) \n",
    "\n",
    "x_train = preprocessor(inputs) ## x_train is a tensor that contains our data\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = tf.keras.utils.to_categorical(list(y_train), num_classes=2)\n",
    "y_eval_binary = tf.keras.utils.to_categorical(list(y_eval), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes, 0 dropout_prob, lr None, batch size 16\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.4594 - accuracy: 0.7955\n",
      "10 nodes, 0 dropout_prob, lr None, batch size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000001B76A71BCA0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mfatt\\anaconda3\\lib\\weakref.py\", line 346, in remove\n",
      "    self = selfref()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 5ms/step - loss: 0.4571 - accuracy: 0.7727\n",
      "10 nodes, 0 dropout_prob, lr None, batch size 64\n"
     ]
    }
   ],
   "source": [
    "# We will create a for loop to test different parameters of our sequential neural network and keep \n",
    "# the one that minimizes the loss\n",
    "\n",
    "## We set some hyperparameters ##\n",
    "least_value_loss = float('inf')\n",
    "least_loss_model = None\n",
    "nn_sequential_least = None\n",
    "epochs = 10\n",
    "validation_split = 0.2\n",
    "N_train = dftrain.shape[0]\n",
    "\n",
    "## We go through the different parameters to find the \"best\" possible model ##\n",
    "## For this we will keep the minimum loss model ##\n",
    "nodes = [10, 15, 20]\n",
    "dropout_probs = [0, 0.2, 0.4]\n",
    "# learning_rate = [0.01, 0.005, 0.001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "# kernel_regularizer_l1 = tf.kears.regularizers.l1(0.0001)\n",
    "kernel_regularizer_l2 = tf.keras.regularizers.l2(0.0001)\n",
    "\n",
    "\n",
    "parameters_best_model = {}\n",
    "\n",
    "\n",
    "for nodes in nodes:\n",
    "    for dropout_prob in dropout_probs:\n",
    "        # for lr in learning_rate:\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            initial_learning_rate = 0.01\n",
    "            decay_steps = (N_train // batch_size) * 2\n",
    "            decay_rate = 1\n",
    "            staircase = False\n",
    "\n",
    "            ## The learning rate decreases with time\n",
    "            lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "                initial_learning_rate,\n",
    "                decay_steps,\n",
    "                decay_rate,\n",
    "                staircase=staircase\n",
    "            )\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "            print(f\"{nodes} nodes, {dropout_prob} dropout_prob, lr {None}, batch size {batch_size}\")\n",
    "            \n",
    "            body = tf.keras.Sequential([\n",
    "                \n",
    "                tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=kernel_regularizer_l2),\n",
    "                tf.keras.layers.Dropout(dropout_prob), ## Dropout is added to regularize\n",
    "                                                            ## the neural network model to avoid overlearning\n",
    "                tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=kernel_regularizer_l2),\n",
    "                tf.keras.layers.Dropout(dropout_prob),\n",
    "                tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=kernel_regularizer_l2),\n",
    "                tf.keras.layers.Dropout(dropout_prob),\n",
    "                tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=kernel_regularizer_l2),\n",
    "                tf.keras.layers.Dropout(dropout_prob),\n",
    "                \n",
    "                tf.keras.layers.Dense(2, activation='softmax')\n",
    "                \n",
    "                ])\n",
    "            \n",
    "            \n",
    "            result = body(x_train)\n",
    "            model = tf.keras.Model(inputs, result)\n",
    "            \n",
    "            model.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy,\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "            history = model.fit(dict(dftrain), y_train_binary, epochs=epochs, \n",
    "                                batch_size=batch_size, validation_split=validation_split, verbose=0)\n",
    "            \n",
    "            value_loss = model.evaluate(dict(dfeval), y_eval_binary, verbose=1)[0]\n",
    "            \n",
    "\n",
    "\n",
    "            if value_loss < least_value_loss:\n",
    "\n",
    "                least_value_loss = value_loss\n",
    "                least_loss_model = model\n",
    "                \n",
    "                nn_sequential_least = body \n",
    "                \n",
    "                parameters_best_model['num_nodes'] = nodes\n",
    "                parameters_best_model['dropout_prob'] = dropout_prob\n",
    "                parameters_best_model['lr'] = None\n",
    "                parameters_best_model['batch_size'] = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The parameters of the model minimizing the loss :\")\n",
    "print()\n",
    "print(parameters_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_403\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1637 (Dense)          (None, 20)                840       \n",
      "                                                                 \n",
      " dropout_1234 (Dropout)      (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1638 (Dense)          (None, 20)                420       \n",
      "                                                                 \n",
      " dropout_1235 (Dropout)      (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1639 (Dense)          (None, 20)                420       \n",
      "                                                                 \n",
      " dropout_1236 (Dropout)      (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1640 (Dense)          (None, 20)                420       \n",
      "                                                                 \n",
      " dropout_1237 (Dropout)      (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1641 (Dense)          (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,142\n",
      "Trainable params: 2,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Taille des données en entrée : (None, 41)\n",
      "\n",
      "Configuration du réseau de neurones: \n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "\n",
      "Taille des données en sortie : (None, 20)\n",
      "(None, 41)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n"
     ]
    }
   ],
   "source": [
    "## Configuration of the neural network\n",
    "\n",
    "# # print(nn_sequential_least.get_config())\n",
    "print(nn_sequential_least.summary())\n",
    "\n",
    "\n",
    "# Récupérer la taille des données d'entrée\n",
    "input_shape = nn_sequential_least.layers[0].input_shape\n",
    "print(\"Taille des données en entrée :\", input_shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Configuration du réseau de neurones: \")\n",
    "for i in range(1, 5):\n",
    "    print(nn_sequential_least.layers[i].input_shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# Récupérer la taille des données d'entrée\n",
    "output_shape = nn_sequential_least.layers[4].output_shape\n",
    "print(\"Taille des données en sortie :\", output_shape)\n",
    "\n",
    "# for i in range(0, 7):\n",
    "#     print(nn_sequential_least.layers[i].input_shape)\n",
    "\n",
    "# print()\n",
    "\n",
    "# for i in range(0, 7):\n",
    "#     print(nn_sequential_least.layers[i].output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4395 - accuracy: 0.8106\n",
      "\n",
      "Eval accuracy: 0.810606062412262\n",
      "\n",
      "Eval loss: 0.43949180841445923\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = least_loss_model.evaluate(dict(dfeval), y_eval_binary)\n",
    "\n",
    "print()\n",
    "print('Eval accuracy:', eval_accuracy)\n",
    "print()\n",
    "print('Eval loss:', eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./sequential_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./sequential_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./sequential_nn_config\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./sequential_nn_config\\assets\n"
     ]
    }
   ],
   "source": [
    "# # Save the best model\n",
    "# least_loss_model.save(filepath=\"./sequential_model\")\n",
    "\n",
    "\n",
    "# # Save the configuration\n",
    "# nn_sequential_least.save(filepath=\"./sequential_nn_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load the best model and the configuration\n",
    "loaded_model = tf.keras.models.load_model(filepath=\"./sequential_model\")\n",
    "\n",
    "loaded_config_nn = tf.keras.models.load_model(filepath=\"./sequential_nn_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_374\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1496 (Dense)          (None, 15)                630       \n",
      "                                                                 \n",
      " dropout_1122 (Dropout)      (None, 15)                0         \n",
      "                                                                 \n",
      " dense_1497 (Dense)          (None, 15)                240       \n",
      "                                                                 \n",
      " dropout_1123 (Dropout)      (None, 15)                0         \n",
      "                                                                 \n",
      " dense_1498 (Dense)          (None, 15)                240       \n",
      "                                                                 \n",
      " dropout_1124 (Dropout)      (None, 15)                0         \n",
      "                                                                 \n",
      " dense_1499 (Dense)          (None, 2)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,142\n",
      "Trainable params: 1,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Taille des données en entrée : (None, 41)\n",
      "\n",
      "Configuration du réseau de neurones: \n",
      "(None, 15)\n",
      "(None, 15)\n",
      "(None, 15)\n",
      "(None, 15)\n",
      "(None, 15)\n",
      "(None, 15)\n",
      "\n",
      "Taille des données en sortie : (None, 2)\n"
     ]
    }
   ],
   "source": [
    "## Configuration of the neural network\n",
    "\n",
    "# print(loaded_config_nn.get_config())\n",
    "print(loaded_config_nn.summary())\n",
    "\n",
    "\n",
    "# Récupérer la taille des données d'entrée\n",
    "input_shape = loaded_config_nn.layers[0].input_shape\n",
    "print(\"Taille des données en entrée :\", input_shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Configuration du réseau de neurones: \")\n",
    "for i in range(1, 7):\n",
    "    print(loaded_config_nn.layers[i].input_shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# Récupérer la taille des données d'entrée\n",
    "output_shape = loaded_config_nn.layers[6].output_shape\n",
    "print(\"Taille des données en sortie :\", output_shape)\n",
    "\n",
    "# for i in range(0, 7):\n",
    "#     print(loaded_config_nn.layers[i].input_shape)\n",
    "\n",
    "# print()\n",
    "\n",
    "# for i in range(0, 7):\n",
    "#     print(loaded_config_nn.layers[i].output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and theirs dimensions : \n",
      "\n",
      "sex : (None,)\n",
      "age : (None,)\n",
      "n_siblings_spouses : (None,)\n",
      "parch : (None,)\n",
      "fare : (None,)\n",
      "class : (None,)\n",
      "deck : (None,)\n",
      "embark_town : (None,)\n",
      "alone : (None,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features and theirs dimensions : \")\n",
    "print()\n",
    "for feature, shape in loaded_model.layers[9].input_shape.items():\n",
    "    print(feature + \" : \" + str(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels\n",
    "y_train_binary = tf.keras.utils.to_categorical(list(y_train), num_classes=2)\n",
    "y_eval_binary = tf.keras.utils.to_categorical(list(y_eval), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 10ms/step - loss: 0.4178 - accuracy: 0.8333\n",
      "\n",
      "Eval accuracy: 0.8333333134651184\n",
      "\n",
      "Eval loss: 0.41780975461006165\n"
     ]
    }
   ],
   "source": [
    "# Modèle retenu: '{'num_nodes': 15, 'dropout_prob': 0, 'lr': 0.005, 'batch_size': 32}\n",
    "\n",
    "eval_loss, eval_accuracy = loaded_model.evaluate(dict(dfeval), y_eval_binary)\n",
    "\n",
    "print()\n",
    "print('Eval accuracy:', eval_accuracy)\n",
    "print()\n",
    "print('Eval loss:', eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dfeval contains the data of 264 passengers present on board \n",
    "# the titanic during the sinking. This function uses the prediction\n",
    "# of the neural network model recorded above and compares it to the true value.\n",
    "\n",
    "def input_and_predict():\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        try: \n",
    "            passenger_number = int(input(\"Enter an integer between 0 and 263.\"))\n",
    "            \n",
    "            if passenger_number >= 0 and passenger_number <= 263: \n",
    "                \n",
    "                print()\n",
    "                \n",
    "                # Perform the prediction for the given passenger\n",
    "                predictions = loaded_model.predict(dict(dfeval))\n",
    "                prediction_passager = predictions[passenger_number]\n",
    "\n",
    "                print()\n",
    "\n",
    "                # Show results\n",
    "                print(\"Passenger data : \")\n",
    "                print(dfeval.iloc[passenger_number, ])\n",
    "\n",
    "                print()\n",
    "\n",
    "                p_1 = round(prediction_passager[1], 2)\n",
    "                p_2 = round(prediction_passager[0], 2)\n",
    "\n",
    "                if p_1 >= 0.5: print(\"Model prediction: Survivor with probability : \", p_1)\n",
    "                else: print(\"Model prediction: Not-survivor with probability : \", p_2)\n",
    "\n",
    "                print()\n",
    "\n",
    "                if y_eval[passenger_number] == 1: survival = \"Survivor\"\n",
    "                else: survival = \"Not-Survivor\"\n",
    "\n",
    "                print(\"Actual value : \", survival)\n",
    "\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print(\"Incorrect value. Enter an integer between 0 and 263.\")\n",
    "            \n",
    "        except ValueError: print(\"Incorrect format. Enter an integer between 0 and 263.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9/9 [==============================] - 1s 5ms/step\n",
      "\n",
      "Passenger data : \n",
      "sex                       female\n",
      "age                         22.0\n",
      "n_siblings_spouses             0\n",
      "parch                          0\n",
      "fare                        7.75\n",
      "class                      Third\n",
      "deck                     unknown\n",
      "embark_town           Queenstown\n",
      "alone                          y\n",
      "Name: 78, dtype: object\n",
      "\n",
      "Model prediction: Survivor with probability :  0.83\n",
      "\n",
      "Actual value :  Survivor\n"
     ]
    }
   ],
   "source": [
    "input_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 5ms/step\n",
      "\n",
      "---------------- classification report ----------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       165\n",
      "           1       0.86      0.67      0.75        99\n",
      "\n",
      "    accuracy                           0.83       264\n",
      "   macro avg       0.84      0.80      0.81       264\n",
      "weighted avg       0.84      0.83      0.83       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Classification report with different metrics\n",
    "\n",
    "predictions = loaded_model.predict(dict(dfeval))\n",
    "y_pred_classes = np.argmax(predictions, axis=1)\n",
    "y_true_binary = np.argmax(y_eval_binary, axis=1)\n",
    "\n",
    "report = classification_report(y_true_binary, y_pred_classes)\n",
    "\n",
    "print()\n",
    "print(\"---------------- classification report ----------------\")\n",
    "print()\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
